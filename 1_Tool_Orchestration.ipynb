{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Baic7IEFw07f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Production-style Tool Orchestration:\n",
        "- Hugging Face embeddings for semantic tool retrieval (LangChain)\n",
        "- LangGraph workflow orchestration\n",
        "- Optional LlamaIndex ObjectIndex retriever\n",
        "\n",
        "Install (minimal):\n",
        "  pip install langchain-core langchain-community langchain-huggingface langgraph faiss-cpu\n",
        "\n",
        "Optional (for LlamaIndex retriever):\n",
        "  pip install llama-index\n",
        "\n",
        "Optional (LangSmith tracing):\n",
        "  export LANGCHAIN_TRACING_V2=true\n",
        "  export LANGCHAIN_API_KEY=...\n",
        "  export LANGCHAIN_PROJECT=\"tool-orchestration\"\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Protocol, Tuple, TypedDict, Literal\n",
        "\n",
        "# --- LangChain core types\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.tools import Tool\n",
        "\n",
        "# --- HF embeddings (LangChain's Hugging Face integration)\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# --- Vector store\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --- LangGraph\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) Tools (examples)\n",
        "# ============================================================\n",
        "\n",
        "def weather_api(city: str) -> str:\n",
        "    return f\"[demo] Weather in {city}: Sunny, 30C\"\n",
        "\n",
        "def fx_rate(base: str, quote: str) -> str:\n",
        "    # demo\n",
        "    return f\"[demo] FX rate {base}/{quote} = 3.67\"\n",
        "\n",
        "def sql_query(query: str) -> str:\n",
        "    # demo\n",
        "    return f\"[demo] Executed SQL: {query}\"\n",
        "\n",
        "\n",
        "WEATHER_TOOL = Tool(\n",
        "    name=\"weather_api\",\n",
        "    description=\"Get current weather for a city. Inputs: city (string).\",\n",
        "    func=lambda city: weather_api(city),\n",
        ")\n",
        "\n",
        "FX_TOOL = Tool(\n",
        "    name=\"fx_rate\",\n",
        "    description=\"Get FX exchange rate between two currencies. Inputs: base, quote (ISO codes).\",\n",
        "    func=lambda base, quote: fx_rate(base, quote),\n",
        ")\n",
        "\n",
        "SQL_TOOL = Tool(\n",
        "    name=\"sql_query\",\n",
        "    description=\"Execute an analytics SQL query (select/aggregate/join) over a database. Input: query (string).\",\n",
        "    func=lambda query: sql_query(query),\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) Tool Registry (stores tools + descriptions)\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class ToolSpec:\n",
        "    tool: Tool\n",
        "    tags: Tuple[str, ...] = ()\n",
        "    # production metadata you might want to include\n",
        "    cost: float = 1.0\n",
        "    latency_ms: int = 100\n",
        "\n",
        "class ToolRegistry:\n",
        "    def __init__(self) -> None:\n",
        "        self._tools: Dict[str, ToolSpec] = {}\n",
        "\n",
        "    def register(self, spec: ToolSpec) -> None:\n",
        "        self._tools[spec.tool.name] = spec\n",
        "\n",
        "    def get(self, name: str) -> ToolSpec:\n",
        "        return self._tools[name]\n",
        "\n",
        "    def all(self) -> List[ToolSpec]:\n",
        "        return list(self._tools.values())\n",
        "\n",
        "    def tool_objects(self) -> List[Tool]:\n",
        "        return [ts.tool for ts in self._tools.values()]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) Retriever interface + (A) LangChain VectorStore retriever\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class RetrievedTool:\n",
        "    name: str\n",
        "    score: float\n",
        "\n",
        "class ToolRetriever(Protocol):\n",
        "    def retrieve(self, query: str, top_k: int = 5, min_score: float = 0.2) -> List[RetrievedTool]:\n",
        "        ...\n",
        "\n",
        "class LangChainVectorToolRetriever:\n",
        "    \"\"\"\n",
        "    Production pattern:\n",
        "    - Embed tool descriptions using HuggingFaceEmbeddings\n",
        "    - Store in FAISS\n",
        "    - Similarity search at runtime (returns relevance scores)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        registry: ToolRegistry,\n",
        "        *,\n",
        "        hf_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    ) -> None:\n",
        "        self.registry = registry\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=hf_model_name)\n",
        "        self.vstore: Optional[FAISS] = None\n",
        "        self._build_index()\n",
        "\n",
        "    def _tool_doc(self, spec: ToolSpec) -> Document:\n",
        "        txt = (\n",
        "            f\"name: {spec.tool.name}\\n\"\n",
        "            f\"description: {spec.tool.description or ''}\\n\"\n",
        "            f\"tags: {', '.join(spec.tags)}\\n\"\n",
        "            f\"cost: {spec.cost}\\n\"\n",
        "            f\"latency_ms: {spec.latency_ms}\\n\"\n",
        "        )\n",
        "        return Document(page_content=txt, metadata={\"tool_name\": spec.tool.name})\n",
        "\n",
        "    def _build_index(self) -> None:\n",
        "        docs = [self._tool_doc(spec) for spec in self.registry.all()]\n",
        "        if not docs:\n",
        "            self.vstore = None\n",
        "            return\n",
        "        self.vstore = FAISS.from_documents(docs, self.embeddings)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5, min_score: float = 0.2) -> List[RetrievedTool]:\n",
        "        if self.vstore is None:\n",
        "            return []\n",
        "\n",
        "        # returns List[Tuple[Document, relevance_score]]\n",
        "        results = self.vstore.similarity_search_with_relevance_scores(query, k=top_k)\n",
        "\n",
        "        out: List[RetrievedTool] = []\n",
        "        for doc, score in results:\n",
        "            name = doc.metadata.get(\"tool_name\")\n",
        "            if not name:\n",
        "                continue\n",
        "            if float(score) >= min_score:\n",
        "                out.append(RetrievedTool(name=name, score=float(score)))\n",
        "\n",
        "        out.sort(key=lambda x: x.score, reverse=True)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) Optional retriever (B) LlamaIndex ObjectIndex retriever\n",
        "# ============================================================\n",
        "\n",
        "class LlamaIndexObjectToolRetriever:\n",
        "    \"\"\"\n",
        "    Optional: Use LlamaIndex ObjectIndex to index/retrieve Tool objects.\n",
        "    This is helpful when you want LlamaIndex-native tool retrieval patterns.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        registry: ToolRegistry,\n",
        "        *,\n",
        "        hf_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        top_k_default: int = 5,\n",
        "    ) -> None:\n",
        "        # Lazy imports so the file runs without llama-index installed\n",
        "        from llama_index.core.objects import ObjectIndex\n",
        "        from llama_index.core.embeddings import BaseEmbedding\n",
        "\n",
        "        # Build a small adapter so LlamaIndex can call the same HF model\n",
        "        # (kept simple; in real prod you might use a native LlamaIndex embedding class)\n",
        "        class _HFEmbedAdapter(BaseEmbedding):\n",
        "            def __init__(self, lc_embeddings: HuggingFaceEmbeddings):\n",
        "                super().__init__()\n",
        "                self.lc = lc_embeddings\n",
        "\n",
        "            def _get_text_embedding(self, text: str) -> List[float]:\n",
        "                return self.lc.embed_query(text)\n",
        "\n",
        "            def _get_query_embedding(self, query: str) -> List[float]:\n",
        "                return self.lc.embed_query(query)\n",
        "\n",
        "            async def _aget_query_embedding(self, query: str) -> List[float]:\n",
        "                return self.lc.embed_query(query)\n",
        "\n",
        "            async def _aget_text_embedding(self, text: str) -> List[float]:\n",
        "                return self.lc.embed_query(text)\n",
        "\n",
        "        self.registry = registry\n",
        "        self.top_k_default = top_k_default\n",
        "        self.lc_embeddings = HuggingFaceEmbeddings(model_name=hf_model_name)\n",
        "        self.li_embed = _HFEmbedAdapter(self.lc_embeddings)\n",
        "\n",
        "        # LlamaIndex wants “objects”; we’ll index ToolSpec text representations but store names.\n",
        "        objects = []\n",
        "        for spec in registry.all():\n",
        "            objects.append(\n",
        "                {\n",
        "                    \"tool_name\": spec.tool.name,\n",
        "                    \"text\": f\"{spec.tool.name}: {spec.tool.description or ''} tags={','.join(spec.tags)}\"\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Build ObjectIndex (simple: index dict objects)\n",
        "        self.object_index = ObjectIndex.from_objects(objects, embed_model=self.li_embed)\n",
        "        self.retriever = self.object_index.as_retriever(similarity_top_k=self.top_k_default)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5, min_score: float = 0.0) -> List[RetrievedTool]:\n",
        "        # LlamaIndex retriever returns nodes with scores; API can vary by version.\n",
        "        # We'll keep it robust.\n",
        "        self.retriever.similarity_top_k = top_k\n",
        "        nodes = self.retriever.retrieve(query)\n",
        "\n",
        "        out: List[RetrievedTool] = []\n",
        "        for n in nodes:\n",
        "            # n.node.metadata may differ; try common places\n",
        "            tool_name = None\n",
        "            score = getattr(n, \"score\", 0.0) or 0.0\n",
        "            node_obj = getattr(n, \"node\", None)\n",
        "\n",
        "            if node_obj is not None:\n",
        "                meta = getattr(node_obj, \"metadata\", None) or {}\n",
        "                tool_name = meta.get(\"tool_name\")\n",
        "\n",
        "                # if we stored dict objects, metadata might be empty; fallback to text parse\n",
        "                if tool_name is None:\n",
        "                    text = getattr(node_obj, \"text\", None) or getattr(node_obj, \"get_text\", lambda: \"\")()\n",
        "                    if isinstance(text, str) and \":\" in text:\n",
        "                        tool_name = text.split(\":\", 1)[0].strip()\n",
        "\n",
        "            if tool_name and score >= min_score and tool_name in [ts.tool.name for ts in self.registry.all()]:\n",
        "                out.append(RetrievedTool(name=tool_name, score=float(score)))\n",
        "\n",
        "        out.sort(key=lambda x: x.score, reverse=True)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) LangGraph orchestration (retrieve -> choose -> execute)\n",
        "# ============================================================\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    user_query: str\n",
        "    # In real systems, you’ll also keep message history + observations\n",
        "    tool_args: Dict[str, Any]        # provided by caller (or produced by an LLM in another node)\n",
        "    candidates: List[RetrievedTool]\n",
        "    chosen_tool: Optional[str]\n",
        "    result: Optional[Any]\n",
        "    error: Optional[str]\n",
        "\n",
        "def retrieve_node(state: AgentState, retriever: ToolRetriever) -> AgentState:\n",
        "    state[\"candidates\"] = retriever.retrieve(state[\"user_query\"], top_k=5, min_score=0.2)\n",
        "    return state\n",
        "\n",
        "def choose_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Production note:\n",
        "    - In real agentic systems, an LLM chooses among candidates and generates args.\n",
        "    - For a coding round, keep it deterministic: pick the top candidate.\n",
        "    \"\"\"\n",
        "    state[\"chosen_tool\"] = state[\"candidates\"][0].name if state[\"candidates\"] else None\n",
        "    state[\"error\"] = None if state[\"chosen_tool\"] else \"No suitable tool found.\"\n",
        "    return state\n",
        "\n",
        "def execute_node(state: AgentState, registry: ToolRegistry) -> AgentState:\n",
        "    name = state.get(\"chosen_tool\")\n",
        "    if not name:\n",
        "        return state\n",
        "\n",
        "    try:\n",
        "        tool = registry.get(name).tool\n",
        "        # Tool.invoke expects a single input; we support dict for multi-arg tools.\n",
        "        tool_input = state.get(\"tool_args\", {})\n",
        "        state[\"result\"] = tool.invoke(tool_input)\n",
        "        state[\"error\"] = None\n",
        "    except Exception as e:\n",
        "        state[\"error\"] = str(e)\n",
        "    return state\n",
        "\n",
        "def build_graph(registry: ToolRegistry, retriever: ToolRetriever):\n",
        "    g = StateGraph(AgentState)\n",
        "    g.add_node(\"retrieve\", lambda s: retrieve_node(s, retriever))\n",
        "    g.add_node(\"choose\", choose_node)\n",
        "    g.add_node(\"execute\", lambda s: execute_node(s, registry))\n",
        "\n",
        "    g.add_edge(START, \"retrieve\")\n",
        "    g.add_edge(\"retrieve\", \"choose\")\n",
        "    g.add_edge(\"choose\", \"execute\")\n",
        "    g.add_edge(\"execute\", END)\n",
        "\n",
        "    return g.compile()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6) Demo\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    registry = ToolRegistry()\n",
        "    registry.register(ToolSpec(tool=WEATHER_TOOL, tags=(\"weather\", \"forecast\"), cost=1.0, latency_ms=120))\n",
        "    registry.register(ToolSpec(tool=FX_TOOL, tags=(\"finance\", \"fx\"), cost=0.5, latency_ms=80))\n",
        "    registry.register(ToolSpec(tool=SQL_TOOL, tags=(\"sql\", \"analytics\", \"database\"), cost=2.0, latency_ms=200))\n",
        "\n",
        "    # Choose ONE retriever:\n",
        "\n",
        "    # (A) LangChain VectorStore retriever (recommended for this coding round)\n",
        "    retriever: ToolRetriever = LangChainVectorToolRetriever(\n",
        "        registry,\n",
        "        hf_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "\n",
        "    # (B) LlamaIndex ObjectIndex retriever (optional)\n",
        "    # retriever = LlamaIndexObjectToolRetriever(\n",
        "    #     registry,\n",
        "    #     hf_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    # )\n",
        "\n",
        "    graph = build_graph(registry, retriever)\n",
        "\n",
        "    # Example run 1\n",
        "    state: AgentState = {\n",
        "        \"user_query\": \"I need the weather forecast for Dubai\",\n",
        "        \"tool_args\": {\"city\": \"Dubai\"},\n",
        "        \"candidates\": [],\n",
        "        \"chosen_tool\": None,\n",
        "        \"result\": None,\n",
        "        \"error\": None,\n",
        "    }\n",
        "    out = graph.invoke(state)\n",
        "    print(\"Chosen:\", out[\"chosen_tool\"])\n",
        "    print(\"Result:\", out[\"result\"])\n",
        "    print(\"Error:\", out[\"error\"])\n",
        "\n",
        "    # Example run 2\n",
        "    state2: AgentState = {\n",
        "        \"user_query\": \"Give me exchange rate from AED to USD\",\n",
        "        \"tool_args\": {\"base\": \"AED\", \"quote\": \"USD\"},\n",
        "        \"candidates\": [],\n",
        "        \"chosen_tool\": None,\n",
        "        \"result\": None,\n",
        "        \"error\": None,\n",
        "    }\n",
        "    out2 = graph.invoke(state2)\n",
        "    print(\"Chosen:\", out2[\"chosen_tool\"])\n",
        "    print(\"Result:\", out2[\"result\"])\n",
        "    print(\"Error:\", out2[\"error\"])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}